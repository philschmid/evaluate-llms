{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LLM as a Judge to evaluate an **RAG application on a custom metric**\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is one of the most popular use cases for LLMs, but it is also one of the most difficult to evaluate. There are common metrics for RAG, but they might not always fit the use case or are to “generic”. We define a new RAG additive metric (3-Point Scale)\n",
    "\n",
    "This 3-point additive metric evaluates RAG system responses based on their adherence to the given context, completeness in addressing all key elements, and relevance combined with conciseness.\n",
    "\n",
    "*Note: This is a completely made-up metric for demonstration purposes only. It is important you define the metrics and criteria based on your use case and importance.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai huggingface_hub datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help improve the model's performance, we define three few-shot examples: a 0-score example, a 1-score example, and a 3-score example. You can find them in the [dataset repository](https://huggingface.co/datasets/zeitgeist-ai/financial-rag-nvidia-sec). For the evaluation data, we will use a synthetic dataset from the [**2023_10 NVIDIA SEC Filings](https://stocklight.com/stocks/us/nasdaq-nvda/nvidia/annual-reports/nasdaq-nvda-2023-10K-23668751.pdf).** This dataset includes a question, answer, and context. We are going to evaluate 50 random samples to see how well the answer performs based on our defined metric. \n",
    "\n",
    "We are going to use the async client `AsyncOpenAI` client to score multiple examples in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "import huggingface_hub\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "# max concurrency\n",
    "sem = asyncio.Semaphore(5)\n",
    "\n",
    "# Initialize the client using the Hugging Face Inference API\n",
    "client = AsyncOpenAI(\n",
    "    base_url=\"https://api-inference.huggingface.co/v1/\",\n",
    "    api_key=huggingface_hub.get_token(),\n",
    ")\n",
    "\n",
    "# Combined async helper method to handle concurrent scoring and\n",
    "async def limited_get_score(dataset):\n",
    "    async def gen(sample):\n",
    "        async with sem:\n",
    "            res = await get_eval_score(sample)\n",
    "            progress_bar.update(1)\n",
    "            return res\n",
    "\n",
    "    progress_bar = tqdm_asyncio(total=len(dataset), desc=\"Scoring\", unit=\"sample\")\n",
    "    tasks = [gen(text) for text in dataset]\n",
    "    responses = await tqdm_asyncio.gather(*tasks)\n",
    "    progress_bar.close()\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate our model, we need to define the `additive_criteria`, `evaluation_steps`, `json_schema`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_examples(examples):\n",
    "    return \"\\n\".join([\n",
    "        f'Question: {ex[\"question\"]}\\nContext: {ex[\"context\"]}\\nAnswer: {ex[\"answer\"]}\\nEvaluation:{ex[\"eval\"]}' \n",
    "        for ex in examples\n",
    "    ])\n",
    "\n",
    "EVALUATION_PROMPT_TEMPLATE = \"\"\"You are an expert judge evaluating the Retrieval Augmented Generation applications. Your task is to evaluate a given answer based on a context and question using the criteria provided below.\n",
    "\n",
    "Evaluation Criteria (Additive Score, 0-5):\n",
    "{additive_criteria}\n",
    "\n",
    "Evaluation Steps:\n",
    "{evaluation_steps}\n",
    "\n",
    "Output format:\n",
    "{json_schema}\n",
    "\n",
    "Examples:\n",
    "{examples}\n",
    "\n",
    "Now, please evaluate the following:\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "Context:\n",
    "{context}\n",
    "Answer:\n",
    "{answer}\n",
    "\"\"\"\n",
    "\n",
    "ADDITIVE_CRITERIA = \"\"\"1. Context: Award 1 point if the answer uses only information provided in the context, without introducing external or fabricated details.\n",
    "2. Completeness: Add 1 point if the answer addresses all key elements of the question based on the available context, without omissions.\n",
    "3. Conciseness: Add a final point if the answer uses the fewest words possible to address the question and avoids redundancy.\"\"\"\n",
    "\n",
    "EVALUATION_STEPS=\"\"\"1. Read the question and provided answer carefully to understand the context.\n",
    "2. Go through each evaluation criterion one by one and assess whether the answer meets the criteria\n",
    "3. Compose your reasoning for each criterion, explaining why you did or did not award a point. Be specific and refer to elements of the answer that influenced your decision. \n",
    "4. For each criterion that the answer meets, add the corresponding point (up to 1 point per criterion). You can only award full points.\n",
    "5. Format your evaluation response according to the specified Output format, ensuring proper JSON syntax with a \"reasoning\" field for your step-by-step explanation and a \"total_score\" field for the calculated total. Review your formatted response. It needs to be valid JSON.\"\"\"\n",
    "\n",
    "JSON_SCHEMA=\"\"\"{\n",
    "  \"reasoning\": \"Your step-by-step explanation for the Evaluation Criteria, why you awarded a point or not.\"\n",
    "  \"total_score\": sum of criteria scores,\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define our `get_eval_score` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "async def get_eval_score(sample):\n",
    "    prompt = EVALUATION_PROMPT_TEMPLATE.format(\n",
    "        additive_criteria=ADDITIVE_CRITERIA,\n",
    "        evaluation_steps=EVALUATION_STEPS,\n",
    "        json_schema=JSON_SCHEMA,\n",
    "        examples=format_examples(few_shot_examples),\n",
    "        question=sample[\"question\"],\n",
    "        context=sample[\"context\"],\n",
    "        answer=sample[\"answer\"]\n",
    "    )\n",
    "    # Comment in if you want to see the prompt\n",
    "    # print(prompt)\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=512,\n",
    "    )\n",
    "    results = response.choices[0].message.content\n",
    "    # Add the evaluation results to the sample\n",
    "    return {**sample, **json.loads(results)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last missing piece is the data. We use the `datasets` library to load our samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limited evaluation of 50 samples\n",
      "Limited evaluation of 3 few-shot examples\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "eval_ds = load_dataset(\"zeitgeist-ai/financial-rag-nvidia-sec\", split=\"train\").shuffle(seed=42).select(range(50))\n",
    "print(f\"Limited evaluation of {len(eval_ds)} samples\")\n",
    "few_shot_examples = load_dataset(\"zeitgeist-ai/financial-rag-nvidia-sec\",\"few-shot-examples\" ,split=\"train\")\n",
    "print(f\"Limited evaluation of {len(few_shot_examples)} few-shot examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How much were the company's debt obligations as of December 31, 2023?\n",
      "Context: The company's debt obligations as of December 31, 2023, totaled $2,299,887 thousand.\n",
      "Answer: $2,299,887 thousand\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.68it/s]2.69sample/s]\n",
      "Scoring: 100%|██████████| 1/1 [00:00<00:00,  2.67sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoing: 1. Context: The answer accurately uses the information provided in the context, specifically mentioning the exact amount of debt obligations as of December 31, 2023. Therefore, it earns 1 point for using the correct context.\n",
      "2. Completeness: The answer addresses the key element of the question, which is the specific amount of the company's debt obligations as of December 31, 2023. It does not omit any necessary information, so it earns 1 point for completeness.\n",
      "3. Conciseness: The answer is concise and directly answers the question without any unnecessary information or redundancy. Therefore, it earns 1 point for conciseness.\n",
      "Total Score: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "sample = [sample for sample in eval_ds.select(range(1))]\n",
    "print(f\"Question: {sample[0]['question']}\\nContext: {sample[0]['context']}\\nAnswer: {sample[0]['answer']}\")\n",
    "print(\"---\" * 10)\n",
    "# change in if you are not in a jupyter notebook\n",
    "# responses = asyncio.run(limited_get_score(sample))\n",
    "responses = await limited_get_score(sample)\n",
    "print(f\"Reasoning: {responses[0]['reasoning']}\\nTotal Score: {responses[0]['total_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, it works and looks good now. Let's evaluate all 50 examples and then calculate our average score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 31.21it/s]9.54sample/s]\n",
      "Scoring: 100%|██████████| 50/50 [00:01<00:00, 31.06sample/s]\n"
     ]
    }
   ],
   "source": [
    "results = await limited_get_score(eval_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score: 2.78\n",
      "Samples with score 0: 2\n"
     ]
    }
   ],
   "source": [
    "# calculate the average score\n",
    "total_score = sum([r[\"total_score\"] for r in results]) / len(results)\n",
    "print(f\"Average Score: {total_score}\")\n",
    "\n",
    "# extract and sample with score 0\n",
    "score_0 = [r for r in results if r[\"total_score\"] == 0]\n",
    "print(f\"Samples with score 0: {len(score_0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. We achieved and average score of 2.78! To better understand why only 2.78 lets look at an example which scored poorly and if that's correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What was the total dollar value of outstanding commercial real estate loans at the end of 2023?\n",
      "Context: The total outstanding commercial real estate loans amounted to $72,878 million at the end of December 2022.\n",
      "Answer: $72.878 billion\n",
      "------------------------------\n",
      "Reasoning: 1. Context: The answer does not use the correct information from the provided context. The context mentions the total outstanding commercial real estate loans at the end of December 2022, but the answer provides a value without specifying the correct year. Therefore, no points are awarded for context.\n",
      "2. Completeness: The answer provides a dollar value, but it does not address the key element of the question, which is the total dollar value at the end of 2023. The context only provides information about 2022, and the answer does not clarify or provide the correct information for 2023. Thus, no points are awarded for completeness.\n",
      "3. Conciseness: The answer is concise, but it does not address the correct question. If the answer had provided a value with a clear statement that the information is not available for 2023, it would have been more accurate. However, in this case, the answer is concise but incorrect.\n",
      "Total Score: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Question: {score_0[0]['question']}\\nContext: {score_0[0]['context']}\\nAnswer: {score_0[0]['answer']}\")\n",
    "print(\"---\" * 10)\n",
    "print(f\"Reasoning: {score_0[0]['reasoning']}\\nTotal Score: {score_0[0]['total_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. Our LLM judge correctly identified that the question asked for 2023, but the context only provided information about 2022. Additionally, we see that the completeness and conciseness criteria really rely heavily on context. Depending on your needs, there could be room for improvements in our prompt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
