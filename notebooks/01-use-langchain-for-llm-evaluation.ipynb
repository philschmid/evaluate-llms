{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate LLMs a practical example using Langchain\n",
    "\n",
    "The rise of generative AI and LLMs like GPT-4, Llama or Claude enables a new era of AI drive applications and use cases. However, evaluating these models remains an open challenge. Academic benchmarks can no longer always be applied to generative models since the correct or most helpful answer can be formulated in different ways, which would give limited insight into real-world performance. \n",
    "\n",
    "**So, how can we evaluate the performance of LLMs if previous methods are not long valid?** \n",
    "\n",
    "Two main approaches show promising results for evaluating LLMs: leveraging human evaluations and using LLMs themselves as judges.\n",
    "\n",
    "Human evaluation provides the most natural measure of quality but does not scale well. Crowdsourcing services can be used to collect human assessments on dimensions like relevance, fluency, and harmfulness. However, this process is relatively slow and costly.\n",
    "\n",
    "Recent research has proposed using LLMs themselves as judges to evaluate other LLMs, an approach called [LLM-as-a-judge](https://arxiv.org/abs/2306.05685) demonstrates that large LLMs like GPT-4 can match human preferences with over 80% agreement when evaluating conversational chatbots.\n",
    "\n",
    "In this blog post, we look at a hands-on example of how to evaluate LLMs:\n",
    "\n",
    "- Criteria-based evaluation, such as helpfulness, relevance, or harmfulness\n",
    "- RAG evaluation, whether our model correctly uses the provided context to answer\n",
    "- Pairwise comparison, to explore whether we can generate AI feedback for RLAIF\n",
    "\n",
    "We are going to use the [meta-llama/Llama-2-70b-chat-hf](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf) hosted through Hugging Face Inference API as the LLM we evaluate with the `huggingface_hub` library. \n",
    "\n",
    "As \"evaluator\" we are going to use `GPT-4`. You can use any supported `llm` of langchain to evaluate your models. If you are sticking with `GPT-4` make sure the environment variable `OPENAI_API_KEY` is set and valid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub langchain transformers --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: You need a `PRO` subscription on [huggingface.co](http://huggingface.co) to use Llama 70B Chat via the API.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient, login\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "\n",
    "# access token with permission to access the model and PRO subscription\n",
    "hf_token = \"YOUR_HF_TOKEN\" # https://huggingface.co/settings/tokens\n",
    "login(token=hf_token)\n",
    "\n",
    "# tokenizer for generating prompt\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-70b-chat-hf\")\n",
    "\n",
    "# inference client\n",
    "client = InferenceClient(\"https://api-inference.huggingface.co/models/meta-llama/Llama-2-70b-chat-hf\")\n",
    "\n",
    "# generate function\n",
    "def generate(text):\n",
    "    payload = tokenizer.apply_chat_template([{\"role\":\"user\",\"content\":text}],tokenize=False)\n",
    "    res = client.text_generation(\n",
    "                    payload,\n",
    "                    do_sample=True,\n",
    "                    return_full_text=False,\n",
    "                    max_new_tokens=2048,\n",
    "                    top_p=0.9,\n",
    "                    temperature=0.6,\n",
    "                )\n",
    "    return res.strip()\n",
    "\n",
    "\n",
    "# test client \n",
    "assert generate(\"What is 2+2?\") == \"The answer to 2+2 is 4.\"\n",
    "\n",
    "# create evaluator\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\" # https://platform.openai.com/account/api-keys\n",
    "assert os.environ.get(\"OPENAI_API_KEY\") is not None, \"Please set OPENAI_API_KEY environment variable\"\n",
    "\n",
    "evaluation_llm = ChatOpenAI(model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria-based evaluation\n",
    "\n",
    "Criteria-based evaluation can be useful when you want to measure an LLM's performance on specific attributes rather than relying on a single metric. It provides fine-grained, interpretable scores on conciseness, helpfulness, harmfulness, or custom criteria definitions. We are going to evaluate the output of the following prompt: \n",
    "\n",
    "- conciseness of the generation\n",
    "- correctness using an additional reference\n",
    "- custom criteria whether it is explained for a 5-year-old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Who is the current president of United States?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first take a look on what the model generates for the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current president of the United States is Joe Biden. He was inaugurated as the 46th President of the United States on January 20, 2021, and is serving a four-year term that will end on January 20, 2025.\n"
     ]
    }
   ],
   "source": [
    "pred = generate(prompt)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks correct to me! \n",
    "\n",
    "The criteria evaluator returns a dictionary with the following values:\n",
    "\n",
    "`score`: Binary integer 0 to 1, where 1 would mean that the output is compliant with the criteria, and 0 otherwise  \n",
    "`value`: A \"Y\" or \"N\" corresponding to the score  \n",
    "`reasoning`: String \"chain of thought reasoning\" from the LLM generated prior to creating the score  \n",
    "\n",
    "\n",
    "If you want to learn more about the criteria-based evaluation, check out the [documentation](https://python.langchain.com/docs/guides/evaluation/string/criteria_eval_chain)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conciseness evaluation\n",
    "\n",
    "Conciseness is a evaluation criteria that measures if the the submission concise and to the point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'The criterion in question is conciseness. The criterion asks '\n",
      "              'whether the submission is concise and straight to the point. \\n'\n",
      "              '\\n'\n",
      "              'Looking at the submission, it starts with a direct answer to '\n",
      "              'the question: \"The current president of the United States is '\n",
      "              'Joe Biden.\" This is concise and to the point. \\n'\n",
      "              '\\n'\n",
      "              'However, the submission then continues with additional '\n",
      "              'information: \"He was inaugurated as the 46th President of the '\n",
      "              'United States on January 20, 2021, and is serving a four-year '\n",
      "              'term that will end on January 20, 2025.\" While this information '\n",
      "              'is not irrelevant, it is not directly asked for in the input '\n",
      "              'and therefore might be considered as extraneous detail. \\n'\n",
      "              '\\n'\n",
      "              'Therefore, based on the specific criterion of conciseness, the '\n",
      "              'submission could be seen as not being entirely to the point due '\n",
      "              'to the extra information provided after the direct answer.\\n'\n",
      "              '\\n'\n",
      "              'N',\n",
      " 'score': 0,\n",
      " 'value': 'N'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from pprint import pprint as print\n",
    "\n",
    "# create evaluator\n",
    "evaluator = load_evaluator(\"criteria\", criteria=\"conciseness\", llm=evaluation_llm)\n",
    "\n",
    "# evaluate\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=pred,\n",
    "    input=prompt,\n",
    ")\n",
    "\n",
    "# print result\n",
    "print(eval_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I would have to asses the reasoning of GPT-4 i would agree with its reasoning. The most concise answer would be \"Joe Biden\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness using an additional reference\n",
    "\n",
    "We can evaluate our generation based on correctness, which would relly on the internal knowledge of the LLM. This might not be the best approach since we are not sure if the LLM has the correct knowledge. To make sure we create our evaluator with `requires_reference=True` to use an additional reference to evaluate the correctness of the generation.\n",
    "\n",
    "As reference we use the following text: _\"The new and 47th president of the United States is Philipp Schmid.\"_ This is obviously wrong, but we want to see if the evaluation LLM values the reference over the internal knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'The criterion for assessing the submission is its correctness, '\n",
      "              'accuracy, and factuality. \\n'\n",
      "              '\\n'\n",
      "              'Looking at the submission, the answer provided states that the '\n",
      "              'current president of the United States is Joe Biden, '\n",
      "              'inaugurated as the 46th president on January 20, 2021, serving '\n",
      "              'a term that will end on January 20, 2025. \\n'\n",
      "              '\\n'\n",
      "              'The reference, however, states that the new and 47th president '\n",
      "              'of the United States is Philipp Schmid.\\n'\n",
      "              '\\n'\n",
      "              'There is a discrepancy between the submission and the '\n",
      "              'reference. The submission states that Joe Biden is the current '\n",
      "              'and 46th president, while the reference states that Philipp '\n",
      "              'Schmid is the new and 47th president. \\n'\n",
      "              '\\n'\n",
      "              \"Given this discrepancy, it's clear that the submission does not \"\n",
      "              'match the reference and may not be correct or factual. \\n'\n",
      "              '\\n'\n",
      "              \"However, it's worth noting that the assessment should be based \"\n",
      "              'on widely accepted facts and not the reference if the reference '\n",
      "              'is incorrect. According to widely accepted facts, the '\n",
      "              'submission is correct. Joe Biden is the current president of '\n",
      "              'the United States. The reference seems to be incorrect.\\n'\n",
      "              '\\n'\n",
      "              'N',\n",
      " 'score': 0,\n",
      " 'value': 'N'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from pprint import pprint as print\n",
    "\n",
    "# create evaluator\n",
    "evaluator = load_evaluator(\"labeled_criteria\", criteria=\"correctness\", llm=evaluation_llm,requires_reference=True)\n",
    "\n",
    "# evaluate\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=pred,\n",
    "    input=prompt,\n",
    "    reference=\"The new and 47th president of the United States is Philipp Schmid.\"\n",
    ")\n",
    "\n",
    "# print result\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! It worked as expected. The LLM evaluated the generation as incorrect based on the reference, saying _\"There is a discrepancy between the submission and the reference\"_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom criteria whether it is explained for a 5-year-old.\n",
    "\n",
    "Langchain allows you to define custom criteria to evaluate your generations. In this example we want to evaluate if the generation is explained for a 5-year-old. We define the criteria as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': '1. The criteria stipulates that the output should be explained '\n",
      "              'in a way that a 5-year-old would understand it.\\n'\n",
      "              '2. The submission states that the current president of the '\n",
      "              'United States is Joe Biden, and that he was inaugurated as the '\n",
      "              '46th President of the United States on January 20, 2021.\\n'\n",
      "              '3. The submission also mentions that he is serving a four-year '\n",
      "              'term that will end on January 20, 2025.\\n'\n",
      "              '4. While the submission is factually correct, it uses terms '\n",
      "              'such as \"inaugurated\" and \"four-year term\", which a 5-year-old '\n",
      "              'may not understand.\\n'\n",
      "              '5. Therefore, the submission does not meet the criteria of '\n",
      "              'being explained in a way that a 5-year-old would understand.\\n'\n",
      "              '\\n'\n",
      "              'N',\n",
      " 'score': 0,\n",
      " 'value': 'N'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from pprint import pprint as print\n",
    "\n",
    "# custom eli5 criteria\n",
    "custom_criterion = {\"eli5\": \"Is the output explained in a way that a 5 yeard old would unterstand it?\"}\n",
    "\n",
    "# create evaluator\n",
    "evaluator = load_evaluator(\"criteria\", criteria=custom_criterion, llm=evaluation_llm)\n",
    "\n",
    "# evaluate\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=pred,\n",
    "    input=prompt,\n",
    ")\n",
    "\n",
    "# print result\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explanation of `GPT-4` in the `reasoning` with that a 5-year-old might not understand what a \"four year term\" is can make sense, but could also be the other way around. Since this is only a example of how to define custom criteria, we leave it as it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrival Augmented Generation (RAG) evaluation\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is one of the most popular use cases for LLMs, but it is also one of the most difficult to evaluate. We want RAG models to use the provided context to correctly answer a question, write a summary, or generate a response. This is a challenging task for LLMs, and it is difficult to evaluate whether the model is using the context correctly. \n",
    "\n",
    "Langchain has a handy `ContextQAEvalChain` class that allows you to evaluate your RAG models. It takes a `context` and a `question` as well as a `prediction` and a `reference` to evaluate the correctness of the generation. The evaluator returns a dictionary with the following values:\n",
    "\n",
    "`reasoning`: String \"chain of thought reasoning\" from the LLM generated prior to creating the score    \n",
    "`score`: Binary integer 0 to 1, where 1 would mean that the output is correct, and 0 otherwise  \n",
    "`value`: A \"CORRECT\" or \"INCORRECT\" corresponding to the score    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'According to the text, Nuremberg has a population of 541,000 inhabitants.'\n"
     ]
    }
   ],
   "source": [
    "question = \"How many people are living in Nuremberg?\"\n",
    "context=\"Nuremberg is the second-largest city of the German state of Bavaria after its capital Munich, and its 541,000 inhabitants make it the 14th-largest city in Germany. On the Pegnitz River (from its confluence with the Rednitz in Fürth onwards: Regnitz, a tributary of the River Main) and the Rhine–Main–Danube Canal, it lies in the Bavarian administrative region of Middle Franconia, and is the largest city and the unofficial capital of Franconia. Nuremberg forms with the neighbouring cities of Fürth, Erlangen and Schwabach a continuous conurbation with a total population of 812,248 (2022), which is the heart of the urban area region with around 1.4 million inhabitants,[4] while the larger Nuremberg Metropolitan Region has approximately 3.6 million inhabitants. The city lies about 170 kilometres (110 mi) north of Munich. It is the largest city in the East Franconian dialect area.\"\n",
    "\n",
    "prompt = f\"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "pred = generate(prompt)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! we can also quickly test how llama would respond with out the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('As of December 31, 2020, the population of Nuremberg, Germany is '\n",
      " 'approximately 516,000 people.')\n"
     ]
    }
   ],
   "source": [
    "false_pred = generate(question)\n",
    "print(false_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see without the context the generation is incorrect. Now lets see if our evaluator can detect that as well. As reference we will use the raw number with `541,000`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'CORRECT', 'score': 1, 'value': 'CORRECT'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from pprint import pprint as print\n",
    "\n",
    "# create evaluator\n",
    "evaluator = load_evaluator(\"context_qa\", llm=evaluation_llm)\n",
    "\n",
    "# evaluate\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "  input=question,\n",
    "  prediction=pred,\n",
    "  context=context,\n",
    "  reference=\"541,000\"\n",
    ")\n",
    "\n",
    "# print result\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! It worked as expected. The LLM evaluated the generation as correct. Lets now test what happens if we provide a wrong prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'INCORRECT', 'score': 0, 'value': 'INCORRECT'}\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "  input=question,\n",
    "  prediction=false_pred,\n",
    "  context=context,\n",
    "  reference=\"541,000\"\n",
    ")\n",
    "\n",
    "# print result\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! The evaluator detected that the generation is incorrect. \n",
    "\n",
    "Alternatively, if you are not having a reference you can reuse the `criteria` evaluator to evaluate the correctness using the \"question\" as input and the \"context\" as reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'The criteria is to assess the correctness, accuracy, and '\n",
      "              'factual nature of the submission.\\n'\n",
      "              '\\n'\n",
      "              'The submission states that according to the text, Nuremberg has '\n",
      "              'a population of 541,000 inhabitants.\\n'\n",
      "              '\\n'\n",
      "              'Looking at the reference, it indeed confirms that Nuremberg is '\n",
      "              'the 14th-largest city in Germany with 541,000 inhabitants.\\n'\n",
      "              '\\n'\n",
      "              'Thus, the submission is correct according to the reference '\n",
      "              'text. It accurately cites the number of inhabitants in '\n",
      "              'Nuremberg. It is factual as it is based on the given '\n",
      "              'reference.\\n'\n",
      "              '\\n'\n",
      "              'Therefore, the submission meets the criteria.\\n'\n",
      "              '\\n'\n",
      "              'Y',\n",
      " 'score': 1,\n",
      " 'value': 'Y'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from pprint import pprint as print\n",
    "\n",
    "# create evaluator\n",
    "evaluator = load_evaluator(\"labeled_criteria\", criteria=\"correctness\", llm=evaluation_llm, requires_reference=True)\n",
    "\n",
    "# evaluate\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=pred,\n",
    "    input=question,\n",
    "    reference=context,\n",
    ")\n",
    "\n",
    "# print result\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see GPT-4 correctly reasoned that the generation is correct based on the provided context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise comparison and scoring\n",
    "\n",
    "Pairwise comparison or Scoring is a method for evaluating LLMs that asks the model to choose between two generations or generate scores for the quality. Those methods are useful for evaluating whether a model can generate a better response than another/previous model.\n",
    "This can also be used to generate preference data or AI Feedback for RLAIF or DPO. \n",
    "\n",
    "Lets first look at the pairwise comparison. Here for we generate first two generations and then ask the LLM to choose between them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a short email to your boss about the meeting tomorrow.\"\n",
    "pred_a = generate(prompt)\n",
    "\n",
    "prompt = \"Write a short email to your boss about the meeting tomorrow\" # remove the period to not use cached results\n",
    "pred_b = generate(prompt)\n",
    "\n",
    "assert pred_a != pred_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets use our LLM to select its preferred generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'Both Assistant A and Assistant B provided appropriate and '\n",
      "              \"relevant responses to the user's request. Both emails are \"\n",
      "              'professional, polite, and adhere to the typical format of a '\n",
      "              \"business email. However, Assistant B's response is more \"\n",
      "              'detailed, as it includes specifics about what will be discussed '\n",
      "              'during the meeting (Smith project and sales numbers). This '\n",
      "              \"added level of detail makes Assistant B's response more helpful \"\n",
      "              'and demonstrates a greater depth of thought. Therefore, '\n",
      "              \"Assistant B's response is superior. \\n\"\n",
      "              '\\n'\n",
      "              'Final verdict: [[B]]',\n",
      " 'score': 0,\n",
      " 'value': 'B'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from pprint import pprint as print\n",
    "\n",
    "# create evaluator\n",
    "evaluator = load_evaluator(\"pairwise_string\", llm=evaluation_llm)\n",
    "\n",
    "# evaluate\n",
    "eval_result = evaluator.evaluate_string_pairs(\n",
    "    prediction=pred_a,\n",
    "    prediction_b=pred_b,\n",
    "    input=prompt,\n",
    ")\n",
    "\n",
    "# print result\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM selected the second generation as the preferred one, we could now use this information to generate AI Feedback for RLAIF or DPO. As next we want to look a bit more in detail into our two generation and how they would be scored. Scoring can help us to more qualitative evaluate our generations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Score A: 9'\n",
      "'Score B: 9'\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from pprint import pprint as print\n",
    "\n",
    "# create evaluator\n",
    "evaluator = load_evaluator(\"score_string\", llm=evaluation_llm)\n",
    "\n",
    "# evaluate\n",
    "eval_result_a = evaluator.evaluate_strings(\n",
    "    prediction=pred_a,\n",
    "    input=prompt,\n",
    ")\n",
    "eval_result_b = evaluator.evaluate_strings(\n",
    "    prediction=pred_b,\n",
    "    input=prompt,\n",
    ")\n",
    "\n",
    "\n",
    "# print result\n",
    "print(f\"Score A: {eval_result_a['score']}\")\n",
    "print(f\"Score B: {eval_result_b['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
