# Evaluate LLMs

This repository focuses on providing examples and tools for evaluating Large Language Models (LLMs). It includes practical implementations using Langchain and custom evaluation metrics to assess LLM performance in various scenarios.

## Available Notebooks

- [01-use-langchain-for-llm-evaluation.ipynb](notebooks/01-use-langchain-for-llm-evaluation.ipynb): Practical example using Langchain for LLM evaluation.
- [02-g-eval-additive-llm-evaluation.ipynb](notebooks/02-g-eval-additive-llm-evaluation.ipynb): Use LLM as a judge to evaluate an RAG application on a custom metric.
- [03-evaluate-llms-on-common-benchmarks.ipynb](notebooks/03-evaluate-llms-on-common-benchmarks.ipynb): Evaluate LLMs on Common Benchmarks using Evaluation Harness and Hugging Face TGI/vLLM
